{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KAN' from 'kan' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KAN\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_dataset, ex_round\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Load the Dataset\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KAN' from 'kan' (unknown location)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from kan import KAN\n",
    "from kan.utils import create_dataset, ex_round\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "df = pd.read_csv('datasets/numcubes.csv')\n",
    "x = torch.tensor(df[['number']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df[['cube']].values, dtype=torch.float32)\n",
    "\n",
    "# Create the dataset dictionary\n",
    "dataset = {\n",
    "    'train_input': x,\n",
    "    'train_label': y,\n",
    "    'test_input': x,   # Using the same data for simplicity in testing accuracy\n",
    "    'test_label': y\n",
    "}\n",
    "\n",
    "# Step 2: Initialize the Model with only one neuron (input to output)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "din = 1\n",
    "dout = 1\n",
    "G = 4\n",
    "K = 4\n",
    "model = KAN(width=[1, 1], grid=G, k=K, seed=0, device=device)  # Single input-output neuron\n",
    "\n",
    "# FLOP calculation function\n",
    "def calculate_kan_flops(din, dout, G, K):\n",
    "    return (din * dout) * (9 * K * (G + 1.5 * K) + 2 * G - 2.5 * K - 1)\n",
    "\n",
    "# Calculate FLOPs for this KAN model\n",
    "flops = calculate_kan_flops(din, dout, G, K)\n",
    "print(f\"Estimated FLOPs for the KAN model: {flops}\")\n",
    "\n",
    "# Visualize the initial model\n",
    "model(dataset['train_input'])\n",
    "model.plot()\n",
    "\n",
    "# Step 3: Train the Model\n",
    "# Training for 500 steps with Adam optimizer and lambda regularization\n",
    "model.fit(dataset, opt=\"Adam\", steps=500, lamb=0.0001)\n",
    "\n",
    "# Visualize the trained model\n",
    "model.plot()\n",
    "\n",
    "# Step 4: Evaluate the Model (Mean Squared Error)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(dataset['test_input'])\n",
    "    mse_loss = torch.nn.functional.mse_loss(predictions, dataset['test_label'])\n",
    "    print(f\"\\nMean Squared Error on Test Data: {mse_loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Display the Learned Symbolic Formula\n",
    "symbolic_formula = ex_round(model.symbolic_formula()[0][0], 4)\n",
    "print(\"Learned symbolic formula:\", symbolic_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FLOPs for the MLP model: 40000\n",
      "Epoch [100/500], Loss: 133952.3594\n",
      "Epoch [200/500], Loss: 60397.2852\n",
      "Epoch [300/500], Loss: 16213.2070\n",
      "Epoch [400/500], Loss: 5709.9429\n",
      "Epoch [500/500], Loss: 2616.2048\n",
      "\n",
      "Mean Squared Error on Test Data: 2599.5886\n",
      "\n",
      "Predictions vs Actual:\n",
      "Number: 1.0, Predicted Square: -30.60, Actual Square: 1.0\n",
      "Number: 2.0, Predicted Square: -5.53, Actual Square: 8.0\n",
      "Number: 3.0, Predicted Square: 29.62, Actual Square: 27.0\n",
      "Number: 4.0, Predicted Square: 78.97, Actual Square: 64.0\n",
      "Number: 5.0, Predicted Square: 137.90, Actual Square: 125.0\n",
      "Number: 6.0, Predicted Square: 226.13, Actual Square: 216.0\n",
      "Number: 7.0, Predicted Square: 323.64, Actual Square: 343.0\n",
      "Number: 8.0, Predicted Square: 497.51, Actual Square: 512.0\n",
      "Number: 9.0, Predicted Square: 742.09, Actual Square: 729.0\n",
      "Number: 10.0, Predicted Square: 998.82, Actual Square: 1000.0\n",
      "Number: 11.0, Predicted Square: 1337.95, Actual Square: 1331.0\n",
      "Number: 12.0, Predicted Square: 1809.37, Actual Square: 1728.0\n",
      "Number: 13.0, Predicted Square: 2280.95, Actual Square: 2197.0\n",
      "Number: 14.0, Predicted Square: 2752.54, Actual Square: 2744.0\n",
      "Number: 15.0, Predicted Square: 3224.12, Actual Square: 3375.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "df = pd.read_csv('datasets/numcubes.csv')\n",
    "x = torch.tensor(df[['number']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df[['cube']].values, dtype=torch.float32)\n",
    "\n",
    "# Step 2: Define the MLP Model with Customizable Parameters\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=10, output_size=1, num_hidden_layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Create the sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize the model with desired parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 1\n",
    "hidden_size = 10000\n",
    "output_size = 1\n",
    "num_hidden_layers = 1\n",
    "model = MLP(input_size, hidden_size, output_size, num_hidden_layers).to(device)\n",
    "\n",
    "# Move data to the device\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# Calculate FLOPs manually\n",
    "def calculate_flops(model, input_size):\n",
    "    flops = 0\n",
    "    for layer in model.model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer_flops = 2 * layer.in_features * layer.out_features\n",
    "            flops += layer_flops\n",
    "    return flops\n",
    "\n",
    "# Display the calculated FLOPs for this model\n",
    "flops = calculate_flops(model, input_size)\n",
    "print(f\"Estimated FLOPs for the MLP model: {flops}\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Train the Model\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()               # Clear gradients\n",
    "    predictions = model(x)              # Forward pass\n",
    "    loss = criterion(predictions, y)    # Calculate loss\n",
    "    loss.backward()                     # Backward pass\n",
    "    optimizer.step()                    # Update weights\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Evaluate the Model (Mean Squared Error)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(x)\n",
    "    mse_loss = criterion(predictions, y)\n",
    "    print(f\"\\nMean Squared Error on Test Data: {mse_loss.item():.4f}\")\n",
    "\n",
    "# Convert predictions to numpy for easy comparison\n",
    "predictions_np = predictions.cpu().numpy()\n",
    "y_np = y.cpu().numpy()\n",
    "\n",
    "# Display Predictions vs Actual Values\n",
    "print(\"\\nPredictions vs Actual:\")\n",
    "for i in range(len(y_np)):\n",
    "    print(f\"Number: {x[i].item()}, Predicted Square: {predictions_np[i][0]:.2f}, Actual Square: {y_np[i][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
